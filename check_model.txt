Using sparse conv:  False
Use prev coupled:  False
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
Using sparse conv:  False
Use prev coupled:  True
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
Using sparse conv:  False
Use prev coupled:  True
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
self.use_bn False
self.use_mvn False
self.use_lrn False
self.dropout_maps False
keep_prob 1.0
MSAUWrapper(
  (msau_net): MSAUNet(
    (blocks): ModuleList(
      (0): UNetBlock(
        (downsamplingblock): DownSamplingUNetBlock(
          (conv_res_list): ModuleList(
            (0): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(8, 8, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(8, 8, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
            (1): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(16, 16, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(16, 16, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
            (2): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(32, 32, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(32, 32, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
            (3): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
          )
          (conv1s): ModuleList(
            (0): DilConv2dBnLrnDrop(
              (conv): Conv2d(98, 8, kernel_size=[3, 3], stride=(1, 1))
              (lrn): LocalResponseNorm(8, alpha=0.0001, beta=0.75, k=1.0)
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (1): DilConv2dBnLrnDrop(
              (conv): Conv2d(8, 16, kernel_size=[3, 3], stride=(1, 1), dilation=(2, 2))
              (lrn): LocalResponseNorm(16, alpha=0.0001, beta=0.75, k=1.0)
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (2): DilConv2dBnLrnDrop(
              (conv): Conv2d(16, 32, kernel_size=[3, 3], stride=(1, 1), dilation=(4, 4))
              (lrn): LocalResponseNorm(32, alpha=0.0001, beta=0.75, k=1.0)
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (3): DilConv2dBnLrnDrop(
              (conv): Conv2d(32, 64, kernel_size=[3, 3], stride=(1, 1), dilation=(8, 8))
              (lrn): LocalResponseNorm(64, alpha=0.0001, beta=0.75, k=1.0)
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
          )
          (relu): ReLU()
          (conv1_1s): ModuleList()
          (layer_attentions): SAWrapperBlock(
            (attention_block): SelfAttentionBlock(
              (f): CustomAttentConv(
                (pad): ConstantPad2d(padding=[0, 0], value=0)
                (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))
              )
              (g): CustomAttentConv(
                (pad): ConstantPad2d(padding=[0, 0], value=0)
                (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))
              )
              (h): CustomAttentConv(
                (pad): ConstantPad2d(padding=[0, 0], value=0)
                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
              )
              (softmax): Softmax(dim=-1)
            )
          )
          (max_pool): MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)
        )
        (upsamplingblock): UpSamplingUNetBlock(
          (conv_res_list): ModuleList(
            (0): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(8, 8, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(8, 8, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
            (1): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(16, 16, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(16, 16, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
            (2): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(32, 32, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(32, 32, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
          )
          (conv1s): ModuleList(
            (0): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(16, 8, kernel_size=[3, 3], stride=[1, 1])
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (1): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(32, 16, kernel_size=[3, 3], stride=[1, 1])
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (2): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(64, 32, kernel_size=[3, 3], stride=[1, 1])
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
          )
          (relu): ReLU()
          (conv1_1s): ModuleList(
            (0): None
            (1): None
            (2): None
          )
          (deconvs): ModuleList(
            (0): Deconv2DBnLrnDrop(
              (conv): ConvTranspose2d(16, 8, kernel_size=[3, 3], stride=(2, 2), padding=(1, 1))
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (1): Deconv2DBnLrnDrop(
              (conv): ConvTranspose2d(32, 16, kernel_size=[3, 3], stride=(2, 2), padding=(1, 1))
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (2): Deconv2DBnLrnDrop(
              (conv): ConvTranspose2d(64, 32, kernel_size=[3, 3], stride=(2, 2), padding=(1, 1))
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
          )
          (layer_attentions): ModuleList()
        )
      )
      (1): UNetBlock(
        (downsamplingblock): DownSamplingUNetBlock(
          (conv_res_list): ModuleList(
            (0): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(8, 8, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(8, 8, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
            (1): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(16, 16, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(16, 16, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
            (2): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(32, 32, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(32, 32, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
            (3): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
          )
          (conv1s): ModuleList(
            (0): DilConv2dBnLrnDrop(
              (conv): Conv2d(5, 8, kernel_size=[3, 3], stride=(1, 1))
              (lrn): LocalResponseNorm(8, alpha=0.0001, beta=0.75, k=1.0)
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (1): DilConv2dBnLrnDrop(
              (conv): Conv2d(8, 16, kernel_size=[3, 3], stride=(1, 1), dilation=(2, 2))
              (lrn): LocalResponseNorm(16, alpha=0.0001, beta=0.75, k=1.0)
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (2): DilConv2dBnLrnDrop(
              (conv): Conv2d(16, 32, kernel_size=[3, 3], stride=(1, 1), dilation=(4, 4))
              (lrn): LocalResponseNorm(32, alpha=0.0001, beta=0.75, k=1.0)
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (3): DilConv2dBnLrnDrop(
              (conv): Conv2d(32, 64, kernel_size=[3, 3], stride=(1, 1), dilation=(8, 8))
              (lrn): LocalResponseNorm(64, alpha=0.0001, beta=0.75, k=1.0)
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
          )
          (relu): ReLU()
          (conv1_1s): ModuleList(
            (0): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(16, 8, kernel_size=[1, 1], stride=[1, 1])
              (activation): ReLU()
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (1): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(32, 16, kernel_size=[1, 1], stride=[1, 1])
              (activation): ReLU()
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (2): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(64, 32, kernel_size=[1, 1], stride=[1, 1])
              (activation): ReLU()
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (3): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(128, 64, kernel_size=[1, 1], stride=[1, 1])
              (activation): ReLU()
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
          )
          (layer_attentions): SAWrapperBlock(
            (attention_block): SelfAttentionBlock(
              (f): CustomAttentConv(
                (pad): ConstantPad2d(padding=[0, 0], value=0)
                (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))
              )
              (g): CustomAttentConv(
                (pad): ConstantPad2d(padding=[0, 0], value=0)
                (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))
              )
              (h): CustomAttentConv(
                (pad): ConstantPad2d(padding=[0, 0], value=0)
                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
              )
              (softmax): Softmax(dim=-1)
            )
          )
          (max_pool): MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)
        )
        (upsamplingblock): UpSamplingUNetBlock(
          (conv_res_list): ModuleList(
            (0): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(8, 8, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(8, 8, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
            (1): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(16, 16, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(16, 16, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
            (2): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(32, 32, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(32, 32, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
          )
          (conv1s): ModuleList(
            (0): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(16, 8, kernel_size=[3, 3], stride=[1, 1])
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (1): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(32, 16, kernel_size=[3, 3], stride=[1, 1])
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (2): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(64, 32, kernel_size=[3, 3], stride=[1, 1])
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
          )
          (relu): ReLU()
          (conv1_1s): ModuleList(
            (0): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(16, 8, kernel_size=[1, 1], stride=[1, 1])
              (activation): ReLU()
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (1): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(32, 16, kernel_size=[1, 1], stride=[1, 1])
              (activation): ReLU()
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (2): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(64, 32, kernel_size=[1, 1], stride=[1, 1])
              (activation): ReLU()
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
          )
          (deconvs): ModuleList(
            (0): Deconv2DBnLrnDrop(
              (conv): ConvTranspose2d(16, 8, kernel_size=[3, 3], stride=(2, 2), padding=(1, 1))
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (1): Deconv2DBnLrnDrop(
              (conv): ConvTranspose2d(32, 16, kernel_size=[3, 3], stride=(2, 2), padding=(1, 1))
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (2): Deconv2DBnLrnDrop(
              (conv): ConvTranspose2d(64, 32, kernel_size=[3, 3], stride=(2, 2), padding=(1, 1))
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
          )
          (layer_attentions): ModuleList()
        )
      )
      (2): UNetBlock(
        (downsamplingblock): DownSamplingUNetBlock(
          (conv_res_list): ModuleList(
            (0): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(8, 8, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(8, 8, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
            (1): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(16, 16, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(16, 16, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
            (2): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(32, 32, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(32, 32, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
            (3): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(64, 64, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
          )
          (conv1s): ModuleList(
            (0): DilConv2dBnLrnDrop(
              (conv): Conv2d(5, 8, kernel_size=[3, 3], stride=(1, 1))
              (lrn): LocalResponseNorm(8, alpha=0.0001, beta=0.75, k=1.0)
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (1): DilConv2dBnLrnDrop(
              (conv): Conv2d(8, 16, kernel_size=[3, 3], stride=(1, 1), dilation=(2, 2))
              (lrn): LocalResponseNorm(16, alpha=0.0001, beta=0.75, k=1.0)
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (2): DilConv2dBnLrnDrop(
              (conv): Conv2d(16, 32, kernel_size=[3, 3], stride=(1, 1), dilation=(4, 4))
              (lrn): LocalResponseNorm(32, alpha=0.0001, beta=0.75, k=1.0)
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (3): DilConv2dBnLrnDrop(
              (conv): Conv2d(32, 64, kernel_size=[3, 3], stride=(1, 1), dilation=(8, 8))
              (lrn): LocalResponseNorm(64, alpha=0.0001, beta=0.75, k=1.0)
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
          )
          (relu): ReLU()
          (conv1_1s): ModuleList(
            (0): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(16, 8, kernel_size=[1, 1], stride=[1, 1])
              (activation): ReLU()
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (1): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(32, 16, kernel_size=[1, 1], stride=[1, 1])
              (activation): ReLU()
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (2): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(64, 32, kernel_size=[1, 1], stride=[1, 1])
              (activation): ReLU()
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (3): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(128, 64, kernel_size=[1, 1], stride=[1, 1])
              (activation): ReLU()
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
          )
          (layer_attentions): SAWrapperBlock(
            (attention_block): SelfAttentionBlock(
              (f): CustomAttentConv(
                (pad): ConstantPad2d(padding=[0, 0], value=0)
                (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))
              )
              (g): CustomAttentConv(
                (pad): ConstantPad2d(padding=[0, 0], value=0)
                (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))
              )
              (h): CustomAttentConv(
                (pad): ConstantPad2d(padding=[0, 0], value=0)
                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
              )
              (softmax): Softmax(dim=-1)
            )
          )
          (max_pool): MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)
        )
        (upsamplingblock): UpSamplingUNetBlock(
          (conv_res_list): ModuleList(
            (0): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(8, 8, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(8, 8, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
            (1): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(16, 16, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(16, 16, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
            (2): MultiConvResidualBlock(
              (conv_res_list): ModuleList(
                (0): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(32, 32, kernel_size=[3, 3], stride=[1, 1])
                  (activation): ReLU()
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
                (1): Conv2dBnLrnDrop(
                  (custom_conv): Conv2d(32, 32, kernel_size=[3, 3], stride=[1, 1])
                  (dropout): Dropout2d(p=0.0, inplace=False)
                )
              )
              (activation): ReLU()
              (relu): ReLU()
            )
          )
          (conv1s): ModuleList(
            (0): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(16, 8, kernel_size=[3, 3], stride=[1, 1])
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (1): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(32, 16, kernel_size=[3, 3], stride=[1, 1])
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (2): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(64, 32, kernel_size=[3, 3], stride=[1, 1])
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
          )
          (relu): ReLU()
          (conv1_1s): ModuleList(
            (0): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(16, 8, kernel_size=[1, 1], stride=[1, 1])
              (activation): ReLU()
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (1): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(32, 16, kernel_size=[1, 1], stride=[1, 1])
              (activation): ReLU()
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (2): Conv2dBnLrnDrop(
              (custom_conv): Conv2d(64, 32, kernel_size=[1, 1], stride=[1, 1])
              (activation): ReLU()
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
          )
          (deconvs): ModuleList(
            (0): Deconv2DBnLrnDrop(
              (conv): ConvTranspose2d(16, 8, kernel_size=[3, 3], stride=(2, 2), padding=(1, 1))
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (1): Deconv2DBnLrnDrop(
              (conv): ConvTranspose2d(32, 16, kernel_size=[3, 3], stride=(2, 2), padding=(1, 1))
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
            (2): Deconv2DBnLrnDrop(
              (conv): ConvTranspose2d(64, 32, kernel_size=[3, 3], stride=(2, 2), padding=(1, 1))
              (dropout): Dropout2d(p=0.0, inplace=False)
            )
          )
          (layer_attentions): ModuleList()
        )
      )
    )
    (end_convs): ModuleList(
      (0): Conv2dBnLrnDrop(
        (custom_conv): Conv2d(8, 5, kernel_size=[4, 4], stride=[1, 1])
        (dropout): Dropout2d(p=0.0, inplace=False)
      )
      (1): Conv2dBnLrnDrop(
        (custom_conv): Conv2d(8, 5, kernel_size=[4, 4], stride=[1, 1])
        (dropout): Dropout2d(p=0.0, inplace=False)
      )
      (2): Conv2dBnLrnDrop(
        (custom_conv): Conv2d(8, 5, kernel_size=[4, 4], stride=[1, 1])
        (dropout): Dropout2d(p=0.0, inplace=False)
      )
    )
  )
  (predictor): Softmax(dim=1)
  (criterion): CrossEntropyLoss()
)
